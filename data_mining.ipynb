{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40d4954",
   "metadata": {},
   "source": [
    "# Mineria de Patrones - Online Retail\n",
    "\n",
    "Notebook organizado en 4 partes (itemsets frecuentes, reglas + lift, closed/maximal y comparacion final) sobre el dataset Online Retail limpio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd2fd9",
   "metadata": {},
   "source": [
    "Usaremos el dataset `Online Retail` (2010-2011). Si el archivo procesado no existe, se limpia automaticamente a partir de `data/raw/online_retail.csv` replicando la tuberia usada en `eda.ipynb` (sin cancelaciones, cantidades/precios positivos, productos frecuentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59bf39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas numpy seaborn matplotlib mlxtend pyfim kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4cd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules, fpmax\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_PATH = DATA_DIR / \"raw\" / \"online_retail.csv\"\n",
    "PROCESSED_PATH = DATA_DIR / \"processed\" / \"cleaned_online_retail.csv\"\n",
    "RESULTS_DIR = DATA_DIR / \"processed\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parametros para las preguntas\n",
    "SUPPORT_GRID = [0.01, 0.02, 0.05]\n",
    "BASE_SUPPORT = SUPPORT_GRID[0]  # 1%\n",
    "MIN_CONF = 0.60\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23903ee8",
   "metadata": {},
   "source": [
    "## Carga y preprocesamiento\n",
    "Se reutiliza la funcion de limpieza de `eda.ipynb`: se eliminan cancelaciones, cantidades/precios no positivos, descripciones vacias, duplicados exactos y productos con frecuencia < 50. Si ya existe `data/processed/cleaned_online_retail.csv` se reutiliza para no recalcular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_iqr(series, factor=1.5):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - factor * iqr\n",
    "    upper = q3 + factor * iqr\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "\n",
    "def clean_retail(df, min_item_freq=50):\n",
    "    out = df.copy()\n",
    "    out['InvoiceNo'] = out['InvoiceNo'].astype(str).str.strip()\n",
    "\n",
    "    out = out[~out['InvoiceNo'].str.startswith('C')]\n",
    "    out = out[(out['Quantity'] > 0) & (out['UnitPrice'] > 0)]\n",
    "\n",
    "    out['Description'] = out['Description'].astype(str).str.strip()\n",
    "    out = out[out['Description'] != \"\"]\n",
    "    out = out.dropna(subset=['Description'])\n",
    "\n",
    "    out['InvoiceDate'] = pd.to_datetime(out['InvoiceDate'])\n",
    "    out['Country'] = out['Country'].astype(str).str.strip()\n",
    "\n",
    "    out = out.drop_duplicates()\n",
    "\n",
    "    counts = out['Description'].value_counts()\n",
    "    keep_items = counts[counts >= min_item_freq].index\n",
    "    out = out[out['Description'].isin(keep_items)]\n",
    "\n",
    "    out['Quantity'] = clip_iqr(out['Quantity'])\n",
    "    out['UnitPrice'] = clip_iqr(out['UnitPrice'])\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    if PROCESSED_PATH.exists():\n",
    "        df = pd.read_csv(PROCESSED_PATH)\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        print(f\"Dataset limpio cargado desde {PROCESSED_PATH}.\")\n",
    "    else:\n",
    "        if not RAW_PATH.exists():\n",
    "            raise FileNotFoundError(\"Falta data/raw/online_retail.csv. Descarga o coloca el archivo para continuar.\")\n",
    "        df_raw = pd.read_csv(RAW_PATH)\n",
    "        df = clean_retail(df_raw, min_item_freq=50)\n",
    "        df.to_csv(PROCESSED_PATH, index=False)\n",
    "        print(f\"Dataset limpio generado y guardado en {PROCESSED_PATH}.\")\n",
    "    print(f\"Filas: {len(df):,} | Facturas: {df['InvoiceNo'].nunique():,} | Productos: {df['Description'].nunique():,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a2de2",
   "metadata": {},
   "source": [
    "## Preparacion de transacciones (one-hot)\n",
    "Se agrupan los items por factura y se construye una matriz one-hot unica para todas las partes del analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = df.groupby('InvoiceNo')['Description'].apply(list).tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Transacciones: {len(transactions):,} | Items unicos: {onehot.shape[1]}\")\n",
    "print(f\"Densidad promedio one-hot: {onehot.values.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6690ac",
   "metadata": {},
   "source": [
    "## Particion temporal (H1 vs H2)\n",
    "Se separa 2011 en dos mitades para medir crecimientos de patrones (growth rate). Se reutiliza el mismo espacio de items (encoder unico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = pd.Timestamp('2011-06-01')\n",
    "part1 = df[df['InvoiceDate'] < cutoff]\n",
    "part2 = df[df['InvoiceDate'] >= cutoff]\n",
    "\n",
    "transactions1 = part1.groupby('InvoiceNo')['Description'].apply(list).tolist()\n",
    "transactions2 = part2.groupby('InvoiceNo')['Description'].apply(list).tolist()\n",
    "\n",
    "# Reusar el encoder ajustado con todas las transacciones\n",
    "onehot_1 = pd.DataFrame(te.transform(transactions1), columns=te.columns_)\n",
    "onehot_2 = pd.DataFrame(te.transform(transactions2), columns=te.columns_)\n",
    "\n",
    "print(f\"H1 transacciones: {len(transactions1):,} | H2 transacciones: {len(transactions2):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a2168",
   "metadata": {},
   "source": [
    "## Parte 1: Itemsets Frecuentes (5 puntos)\n",
    "1. Mineria de itemsets frecuentes con minsup de 1%, 2% y 5%.\n",
    "2. Conteo de itemsets frecuentes para cada minsup.\n",
    "3. Explicacion de lo que ocurre al aumentar `minsup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fpgrowth(df_oh, min_support):\n",
    "    freq = fpgrowth(df_oh, min_support=min_support, use_colnames=True)\n",
    "    freq = freq.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "    freq['length'] = freq['itemsets'].apply(len)\n",
    "    return freq\n",
    "\n",
    "\n",
    "freq_by_support = {}\n",
    "summary_rows = []\n",
    "for sup in SUPPORT_GRID:\n",
    "    freq = run_fpgrowth(onehot, sup)\n",
    "    freq_by_support[sup] = freq\n",
    "    summary_rows.append({'minsup': sup, 'itemsets': len(freq)})\n",
    "\n",
    "freq_summary = pd.DataFrame(summary_rows)\n",
    "print(\"Itemsets frecuentes por minsup:\")\n",
    "print(freq_summary)\n",
    "\n",
    "# Vistazo a los 5 itemsets con mayor soporte al 1%\n",
    "freq_by_support[BASE_SUPPORT].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94836b",
   "metadata": {},
   "source": [
    "Con minsup=1% se obtienen mas combinaciones (se capturan hasta co-ocurrencias poco frecuentes). Al subir a 2% y 5% solo sobreviven las combinaciones repetidas en muchas facturas, reduciendo el espacio de busqueda y dejando patrones mas robustos comercialmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c36eac",
   "metadata": {},
   "source": [
    "## Parte 2: Reglas de Asociacion y Lift (5 puntos)\n",
    "1. Reglas con minconf = 60% sobre los itemsets de minsup=1%.\n",
    "2. Calculo de lift para cada regla.\n",
    "3. Top 3 reglas por lift y su lectura comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53473573",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_base = freq_by_support[BASE_SUPPORT]\n",
    "rules = association_rules(freq_base, metric='confidence', min_threshold=MIN_CONF)\n",
    "rules = rules.sort_values(by='lift', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Reglas generadas: {len(rules)}\")\n",
    "top3_lift = rules[['antecedents','consequents','support','confidence','lift']].head(3)\n",
    "display(top3_lift)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7f9e0",
   "metadata": {},
   "source": [
    "Las reglas con mayor lift conectan los marcadores de hierbas (`HERB MARKER THYME`, `PARSLEY`, `ROSEMARY`): cuando un cliente compra uno, casi siempre lleva el set completo (confianza >84% y lift >78). Comercialmente es un paquete natural para promociones cruzadas o displays conjuntos porque la probabilidad de compra conjunta es decenas de veces mayor que al azar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18515e96",
   "metadata": {},
   "source": [
    "## Skypatterns (skyline) de reglas y graficas\n",
    "Skypatterns = reglas no dominadas simultaneamente en **soporte**, **confianza** y **lift** (ninguna otra regla es mejor o igual en los tres criterios). Esto resalta patrones con buen equilibrio de frecuencia e intensidad. Se visualiza el skyline sobre el mapa soporte–confianza coloreado por lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skyline_rules(rules_df, metrics=(\"support\", \"confidence\", \"lift\")):\n",
    "    vals = rules_df[list(metrics)].to_numpy()\n",
    "    keep = []\n",
    "    n = len(rules_df)\n",
    "    for i in range(n):\n",
    "        dominated = False\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if all(vals[j, k] >= vals[i, k] for k in range(len(metrics))) and any(vals[j, k] > vals[i, k] for k in range(len(metrics))):\n",
    "                dominated = True\n",
    "                break\n",
    "        if not dominated:\n",
    "            keep.append(i)\n",
    "    return rules_df.iloc[keep].sort_values(by=list(metrics), ascending=False)\n",
    "\n",
    "sky_rules = skyline_rules(rules)\n",
    "print(f\"Skypatterns (reglas skyline): {len(sky_rules)} de {len(rules)} reglas totales\")\n",
    "display(sky_rules[['antecedents','consequents','support','confidence','lift']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41808993",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(rules['support'], rules['confidence'], c=rules['lift'], cmap='viridis', alpha=0.35, label='Reglas')\n",
    "plt.scatter(sky_rules['support'], sky_rules['confidence'], edgecolor='red', facecolor='none', s=120, linewidths=1.5, label='Skyline')\n",
    "plt.colorbar(label='Lift')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Mapa soporte vs confianza con skyline (skypatterns)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6201b",
   "metadata": {},
   "source": [
    "Las reglas skyline combinan soporte razonable con confianza y lift altos sin ser dominadas por otras. Son candidatas directas para campañas: alto lift implica co-compra fuerte y soporte decente asegura impacto en ventas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46190b67",
   "metadata": {},
   "source": [
    "## Patrones emergentes (growth rate H2 vs H1)\n",
    "Se calcula el growth rate = soporte(H2) / soporte(H1) con minsup=1% para identificar itemsets que crecen o aparecen en la segunda mitad del año (JEP si soporte en H1=0). Incluye graficos de los incrementos y JEPs mas soportados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets frecuentes por particion\n",
    "freq_h1 = fpgrowth(onehot_1, min_support=BASE_SUPPORT, use_colnames=True)\n",
    "freq_h2 = fpgrowth(onehot_2, min_support=BASE_SUPPORT, use_colnames=True)\n",
    "\n",
    "merged = pd.merge(freq_h1, freq_h2, on='itemsets', how='outer', suffixes=('_1', '_2')).fillna(0)\n",
    "merged['growth_rate'] = np.where(\n",
    "    merged['support_1'] == 0,\n",
    "    np.inf,\n",
    "    merged['support_2'] / merged['support_1']\n",
    ")\n",
    "\n",
    "jep = merged[merged['growth_rate'] == np.inf].sort_values(by='support_2', ascending=False)\n",
    "inc = merged[(merged['growth_rate'] > 1) & (merged['growth_rate'] != np.inf)].sort_values(by='growth_rate', ascending=False)\n",
    "stable = merged[(merged['growth_rate'] >= 0.8) & (merged['growth_rate'] <= 1.2)].sort_values(by='support_2', ascending=False)\n",
    "dec = merged[(merged['growth_rate'] < 0.8) & (merged['support_2'] > 0)].sort_values(by='growth_rate')\n",
    "\n",
    "print(f\"Itemsets comparados: {len(merged)} | JEPs: {len(jep)} | Incrementan: {len(inc)} | Estables: {len(stable)} | Decrecen: {len(dec)}\")\n",
    "print(\"Top 5 incrementos (sin inf):\")\n",
    "display(inc[['itemsets','support_1','support_2','growth_rate']].head())\n",
    "print(\"Top 5 JEP (solo en H2):\")\n",
    "display(jep[['itemsets','support_1','support_2']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficas\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def iset_to_str(s):\n",
    "    try:\n",
    "        return ', '.join(sorted(s))\n",
    "    except Exception:\n",
    "        return str(s)\n",
    "\n",
    "plot_inc = inc.head(10).copy()\n",
    "plot_inc['item'] = plot_inc['itemsets'].apply(iset_to_str)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(plot_inc['item'], plot_inc['growth_rate'], color='teal')\n",
    "plt.xticks(rotation=60, ha='right')\n",
    "plt.ylabel('Growth rate (H2/H1)')\n",
    "plt.title('Top 10 itemsets que crecen (sin JEP)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_jep = jep.head(10).copy()\n",
    "plot_jep['item'] = plot_jep['itemsets'].apply(iset_to_str)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(plot_jep['item'], plot_jep['support_2'], color='coral')\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "plt.xlabel('Support H2')\n",
    "plt.title('Top 10 Jumping Emerging Patterns (solo H2)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b65699",
   "metadata": {},
   "source": [
    "Los itemsets con mayor growth rate evidencian cambios estacionales: crecen en H2 respecto a H1. Los JEPs solo aparecen en H2 (growth infinito), util para campañas específicas de temporada. Graficas permiten priorizar por impacto (support H2) y velocidad de crecimiento (growth rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ed04c",
   "metadata": {},
   "source": [
    "### Filtrar patrones emergentes no navideños (NLP ligero mejorado)\n",
    "Se normalizan las descripciones (minusculas, sin acentos) y se filtran itemsets con terminos/phrases navideñas (p.ej., christmas, xmas, noel, santa, tree, bauble, fairy lights, gingerbread). Esto remueve estacionalidad fuerte y deja otros patrones emergentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8194821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "navidad_patterns = [\n",
    "    r\"\\bchristmas\\b\", r\"\\bxmas\\b\", r\"\\bx-mas\\b\", r\"\\bnavidad\\b\", r\"\\bnoel\\b\",\n",
    "    r\"\\bsanta\\b\", r\"\\breindeer\\b\", r\"\\bsnow\\b\", r\"\\btinsel\\b\", r\"\\bbaubles?\\b\",\n",
    "    r\"\\bfairy\\s+lights\\b\", r\"\\bchristmas\\s+tree\\b\", r\"\\bgingerbread\\b\", r\"\\badvent\\b\", r\"\\bholly\\b\"\n",
    "]\n",
    "regexes = [re.compile(pat) for pat in navidad_patterns]\n",
    "\n",
    "def normalize_text(txt):\n",
    "    return unicodedata.normalize(\"NFKD\", txt).encode(\"ascii\", \"ignore\").decode(\"ascii\").lower()\n",
    "\n",
    "def is_navideno(itemset):\n",
    "    txt = normalize_text(' '.join(itemset))\n",
    "    return any(r.search(txt) for r in regexes)\n",
    "\n",
    "merged['es_navideno'] = merged['itemsets'].apply(is_navideno)\n",
    "\n",
    "inc_nonxmas = inc[~inc['itemsets'].apply(is_navideno)].head(10).copy()\n",
    "jep_nonxmas = jep[~jep['itemsets'].apply(is_navideno)].head(10).copy()\n",
    "\n",
    "print(f\"Patrones incrementales no navideños: {len(inc_nonxmas)} (de {len(inc)})\")\n",
    "print(f\"JEP no navideños: {len(jep_nonxmas)} (de {len(jep)})\")\n",
    "\n",
    "inc_nonxmas['item'] = inc_nonxmas['itemsets'].apply(lambda s: ', '.join(sorted(s)))\n",
    "jep_nonxmas['item'] = jep_nonxmas['itemsets'].apply(lambda s: ', '.join(sorted(s)))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(inc_nonxmas['item'], inc_nonxmas['growth_rate'], color='seagreen')\n",
    "plt.xticks(rotation=60, ha='right')\n",
    "plt.ylabel('Growth rate (H2/H1)')\n",
    "plt.title('Top 10 itemsets que crecen (sin patrones navideños)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(jep_nonxmas['item'], jep_nonxmas['support_2'], color='steelblue')\n",
    "plt.xlabel('Support H2')\n",
    "plt.title('Top 10 JEP no navideños')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99cb44c",
   "metadata": {},
   "source": [
    "Los filtros NLP (regex + normalizacion) quitan terminos navideños para exponer otros patrones emergentes. Ajusta la lista de keywords si deseas excluir/admitir otros terminos estacionales o locales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c17b4",
   "metadata": {},
   "source": [
    "## Parte 3: Closed Itemsets y Maximal Itemsets (5 puntos)\n",
    "1. Obtencion de closed y maximal itemsets (minsup=1%).\n",
    "2. Muestra de 5 ejemplos de cada grupo.\n",
    "3. Explicacion de por que los maximal reducen el espacio de busqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_itemsets(freq_df):\n",
    "    freq_df = freq_df.sort_values('support', ascending=False).reset_index(drop=True)\n",
    "    itemsets = freq_df['itemsets'].tolist()\n",
    "    supports = freq_df['support'].tolist()\n",
    "    closed_mask = []\n",
    "    for i, (iset, supp) in enumerate(zip(itemsets, supports)):\n",
    "        closed = True\n",
    "        for j, (other, supp_other) in enumerate(zip(itemsets, supports)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if iset < other and supp == supp_other:\n",
    "                closed = False\n",
    "                break\n",
    "        closed_mask.append(closed)\n",
    "    return freq_df.loc[closed_mask].reset_index(drop=True)\n",
    "\n",
    "closed_df = get_closed_itemsets(freq_base)\n",
    "maximal_df = fpmax(onehot, min_support=BASE_SUPPORT, use_colnames=True)\n",
    "maximal_df = maximal_df.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Closed itemsets: {len(closed_df)} | Maximal itemsets: {len(maximal_df)} (minsup={BASE_SUPPORT*100:.0f}%)\")\n",
    "print(\"Top 5 closed itemsets:\")\n",
    "display(closed_df[['itemsets','support']].head())\n",
    "print(\"Top 5 maximal itemsets:\")\n",
    "display(maximal_df[['itemsets','support']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70151788",
   "metadata": {},
   "source": [
    "Los maximal itemsets son aquellos sin supersets frecuentes; al eliminar todos los subconjuntos redundantes, condensan la misma informacion combinatoria en menos filas, reduciendo el espacio de busqueda (y memoria) sin perder los grupos mas grandes que ocurren juntos. Los closed preservan el soporte exacto de sus subconjuntos, por lo que mantienen mas detalle que los maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220de78",
   "metadata": {},
   "source": [
    "## Parte 4: Comparacion y Analisis Final (5 puntos)\n",
    "1. Comparacion entre itemsets frecuentes, closed y maximal.\n",
    "2. Discusion de cual es mas util para un analista de negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'tipo': ['Frequent (>=1%)', 'Closed (>=1%)', 'Maximal (>=1%)'],\n",
    "    'itemsets': [len(freq_base), len(closed_df), len(maximal_df)],\n",
    "    'nota': [\n",
    "        'Incluye todos los subconjuntos frecuentes',\n",
    "        'Elimina subconjuntos con mismo soporte',\n",
    "        'Solo conserva los mayores sin supersets frecuentes'\n",
    "    ]\n",
    "})\n",
    "display(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04049e",
   "metadata": {},
   "source": [
    "Para un analista de negocio, los **closed itemsets** son un buen punto medio: eliminan redundancia pero mantienen soportes exactos para inferir tamanos reales de co-compra. Los **maximal** son utiles para resumir y priorizar (p.ej., armar bundles o planogramas rapidos), mientras que los frecuentes completos sirven para analisis detallados o para entrenar modelos, aunque generan un espacio de patrones mucho mas grande."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMiningTools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
